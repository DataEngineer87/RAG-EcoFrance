##########
import streamlit as st
import faiss
import numpy as np
import json
from huggingface_hub import InferenceClient

# --- Config ---
HUGGINGFACE_API_KEY = "hf_xxxxx"  # ‚ö†Ô∏è Mets ton vrai token ici ou dans les secrets Streamlit
client = InferenceClient(api_key=HUGGINGFACE_API_KEY)

embedding_model = "sentence-transformers/all-MiniLM-L6-v2"

# --- Charger l‚Äôindex FAISS et les docs ---
index = faiss.read_index("Scripts/docs.index")

with open("Scripts/docs.json", "r", encoding="utf-8") as f:
    docs = json.load(f)

# --- Fonctions ---
def embed_query(query: str):
    """
    Cr√©e un embedding via Hugging Face Inference API (feature_extraction).
    Compatible avec Faiss.
    """
    resp = client.feature_extraction(model=embedding_model, inputs=query)
    emb_array = np.array(resp, dtype="float32").reshape(1, -1)  # 2D pour FAISS
    return emb_array

def retrieve_context(query, k=4):
    qv = embed_query(query)
    D, I = index.search(qv, k=k)
    results = []
    for dist, idx in zip(D[0], I[0]):
        if idx < len(docs):  # S√©curit√©
            results.append((dist, docs[idx]))
    return results

# --- UI ---
st.title("üîé RAG EcoFrance")

question = st.text_input("Pose ta question :")
k = st.slider("Nombre de documents √† r√©cup√©rer", 1, 10, 4)
submit = st.button("Rechercher")

if submit and question.strip():
    with st.spinner("üîé Recherche dans l'index et g√©n√©ration de la r√©ponse..."):
        # R√©cup√©rer le contexte via Faiss
        retrieved = retrieve_context(question, k=k)

        # Afficher le contexte utilis√©
        st.subheader("üìö Contexte utilis√©")
        for dist, doc in retrieved:
            st.write(f"**Score:** {dist:.4f}")
            st.write(doc)
            st.markdown("---")
