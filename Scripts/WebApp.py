# WebApp.py (Chatbot RAG avec Hugging Face API)
# Pr√©requis :
#  - docs.index et docs.json cr√©√©s par build_index.py
#  - Hugging Face API key ajout√©e dans .env ou dans Streamlit secrets

import os
import json
import numpy as np
import faiss
import streamlit as st
from huggingface_hub import InferenceClient
from dotenv import load_dotenv

# =====================
# CONFIG
# =====================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
index_file = os.path.join(BASE_DIR, "docs.index")
docs_file = os.path.join(BASE_DIR, "docs.json")

embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
llm_model = "mistralai/Mistral-7B-Instruct-v0.2"
k = 4

# Charger la cl√© API
load_dotenv()
HUGGINGFACE_API_KEY = os.getenv("HUGGINGFACE_API_KEY")
if not HUGGINGFACE_API_KEY:
    st.error("‚ùå Cl√© Hugging Face API manquante. Ajoutez-la dans le .env ou les secrets de Streamlit.")
    st.stop()

client = InferenceClient(token=HUGGINGFACE_API_KEY)

# =====================
# Charger index et documents
# =====================
@st.cache_resource
def load_index_and_docs():
    if not os.path.exists(index_file):
        st.error(f"‚ùå Fichier introuvable : {index_file}")
        st.stop()
    if not os.path.exists(docs_file):
        st.error(f"‚ùå Fichier introuvable : {docs_file}")
        st.stop()
    
    index = faiss.read_index(index_file)
    with open(docs_file, "r", encoding="utf-8") as f:
        docs = json.load(f)
    return index, docs

index, docs = load_index_and_docs()

# =====================
# UI Streamlit
# =====================
st.set_page_config(page_title="Chatbot RAG", page_icon="ü§ñ", layout="centered")

st.markdown("<h3 style='text-align:center;'>ü§ñ CHATBOT RAG - ECONOMIE FRAN√áAISE</h3>", unsafe_allow_html=True)
st.markdown("<h4 style='text-align:center;'>Que souhaitez-vous savoir sur l'√©conomie fran√ßaise ?</h4>", unsafe_allow_html=True)
st.divider()

question = st.text_input("üí¨ Entrez votre question :", placeholder="Ex: Quels sont les facteurs influen√ßant le ch√¥mage ?")
submit = st.button("üöÄ Envoyer")

# =====================
# FONCTIONS
# =====================
def embed_query(query: str):
    """
    G√©n√®re un embedding via Hugging Face Inference API pour Faiss.
    """
    resp = client.post(
        f"https://api-inference.huggingface.co/pipeline/feature-extraction/{embedding_model}",
        json={"inputs": query}
    )
    emb_array = np.array(resp, dtype="float32").reshape(1, -1)
    return emb_array

def retrieve_context(query, k=4):
    qv = embed_query(query)
    D, I = index.search(qv, k=k)
    results = []
    for dist, idx in zip(D[0], I[0]):
        text = docs[idx]
        results.append({"id": int(idx), "distance": float(dist), "text": text})
    return results

def build_prompt(question, retrieved):
    system = "Tu es un assistant expert en √©conomie fran√ßaise. R√©ponds en fran√ßais, cite les sections utilis√©es si utile."
    context = "\n\n---\n".join(
        [f"[chunk id={r['id']} | dist={r['distance']:.4f}]\n{r['text']}" for r in retrieved]
    )
    prompt = f"""{system}

Contexte r√©cup√©r√© (extraits pertinents) :
{context}

Question : {question}

R√©ponds de mani√®re claire et concise en t'appuyant sur le contexte. 
Si l'information n'est pas dans le contexte, dis-le et propose comment l'obtenir."""
    return prompt

def generate_answer(prompt: str):
    output = client.text_generation(model=llm_model, inputs=prompt, max_new_tokens=300)
    if isinstance(output, list) and len(output) > 0:
        return output[0].get("generated_text", "")
    return str(output)

# =====================
# CHAT
# =====================
if submit and question.strip():
    with st.spinner("üîé Recherche dans l'index et g√©n√©ration de la r√©ponse..."):
        retrieved = retrieve_context(question, k=k)

        st.subheader("üìö Contexte utilis√©")
        for r in retrieved:
            with st.expander(f"Chunk {r['id']} (distance={r['distance']:.4f})"):
                st.write(r['text'])

        prompt = build_prompt(question, retrieved)
        answer = generate_answer(prompt)

        st.subheader("ü§ñ R√©ponse du mod√®le")
        st.markdown(
            f"""
            <div style="border:2px solid #4CAF50; padding:15px; border-radius:10px; background-color:#f9fff9;">
                {answer}
            </div>
            """,
            unsafe_allow_html=True,
        )
