import os
import numpy as np
import faiss
import streamlit as st
from huggingface_hub import HfInference
from dotenv import load_dotenv

# =====================
# ğŸ”§ Configuration
# =====================
load_dotenv()
hf_token = os.environ.get("HUGGINGFACE_API_KEY")
embedding_model = os.environ.get("EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2")

if not hf_token:
    st.error("âŒ ClÃ© Hugging Face API manquante. Ajoute-la dans tes secrets ou .env.")
    st.stop()

# Client Hugging Face Inference
client = HfInference(token=hf_token)

# =====================
# ğŸ“š Fonctions
# =====================
def embed_query(query: str):
    """
    CrÃ©e un embedding via Hugging Face et renvoie un np.array float32 2D
    Compatible avec Faiss.
    """
    emb = client.embed_text(model=embedding_model, text=query)
    emb_array = np.array(emb, dtype="float32").reshape(1, -1)
    return emb_array

def retrieve_context(query, k=4):
    """
    RÃ©cupÃ¨re les k passages les plus proches avec Faiss.
    """
    qv = embed_query(query)
    D, I = index.search(qv, k=k)
    results = []
    for dist, idx in zip(D[0], I[0]):
        if idx != -1:
            results.append(docs[idx])
    return results

# =====================
# ğŸ“– Chargement index
# =====================
try:
    index = faiss.read_index("faiss_index.bin")
    with open("docs_store.txt", "r", encoding="utf-8") as f:
        docs = [line.strip() for line in f.readlines()]
except Exception as e:
    st.error(f"âŒ Impossible de charger l'index : {e}")
    st.stop()

# =====================
# ğŸ¨ Interface Streamlit
# =====================
st.set_page_config(page_title="RAG EcoFrance", page_icon="ğŸŒ±", layout="wide")
st.title("ğŸŒ± RAG EcoFrance")
st.write("Pose ta question, je vais chercher dans la base et gÃ©nÃ©rer une rÃ©ponse.")

question = st.text_input("â“ Pose ta question :")
k = st.slider("ğŸ“Š Nombre de passages Ã  rÃ©cupÃ©rer :", 1, 10, 4)
submit = st.button("ğŸ” Rechercher")

if submit and question.strip():
    with st.spinner("ğŸ” Recherche dans l'index et gÃ©nÃ©ration de la rÃ©ponse..."):
        retrieved = retrieve_context(question, k=k)

        st.subheader("ğŸ“š Contexte utilisÃ©")
        for r in retrieved:
            st.markdown(f"- {r}")

        st.subheader("ğŸ¤– RÃ©ponse (placeholder)")
        st.info("ğŸ‘‰ Ici tu peux ajouter lâ€™appel Ã  un modÃ¨le gÃ©nÃ©ratif (Ollama, OpenAI, etc.)")
