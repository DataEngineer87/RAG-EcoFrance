{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "327bdc83-14e8-48f6-a882-847e4846e8c4",
   "metadata": {},
   "source": [
    "# Chatbot RAG - Économie Française\n",
    "Ce projet porte la mise en place d’un chatbot intelligent capable d’interroger un rapport PDF en langage naturel.\n",
    "L’architecture repose sur une approche RAG (Retrieval-Augmented Generation), qui combine la recherche d’informations pertinentes dans un document et la génération de réponses contextuelles grâce à un modèle de langage.\n",
    "\n",
    "Technologies utilisées :\n",
    "\n",
    "- Ollama (Mistral) : génération d’embeddings et production de réponses en français\n",
    "\n",
    "- FAISS : moteur de recherche vectorielle pour retrouver les passages les plus pertinents du document\n",
    "\n",
    "- Streamlit : interface web interactive et conviviale permettant d’interagir avec le chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f58f4-f1dc-4a3f-8229-21dc755a2be2",
   "metadata": {},
   "source": [
    "## Création des fichiers docs.index et docs.json pour l’indexation des documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d13cbc29-2772-4f2f-8278-ed8ee351dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage: exécution après avoir lancé \"ollama serve\" et téléchargé le modèle mistral.\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import ollama\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm  # pas obligatoir mais pratique pour voir l'avancement\n",
    "\n",
    "# Configuration \n",
    "pdf_path = \"/home/sacko/Documents/ChatbotEcoFranceRAG/Fichiers/ECO_FRANCE.pdf\"   # chemin vers le fichier PDF\n",
    "model = \"mistral\"\n",
    "chunk_size = 800       # taille en caractères par chunk \n",
    "chunk_overlap = 200    # recouvrement entre chunks\n",
    "index_file = \"docs.index\"\n",
    "docs_file = \"docs.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da85fabb-0fd0-43d2-a833-53cfd68e9b35",
   "metadata": {},
   "source": [
    "## Extraction automatique du contenu d’un rapport PDF pour l’analyse IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc265ca-ef50-47f5-849a-636c35ba9526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte extrait : 24313 caractères, 13 pages\n"
     ]
    }
   ],
   "source": [
    "# Extraction du contenu textuel d’un PDF\n",
    "reader = PdfReader(pdf_path)\n",
    "pages = [p.extract_text() for p in reader.pages]\n",
    "pages = [p for p in pages if p]  # Exclusion des pages vides lors de l’extraction\n",
    "full_text = \"\\n\\n\".join(pages)\n",
    "print(f\"Texte extrait : {len(full_text)} caractères, {len(pages)} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94eabaf-0248-4bd7-b596-e1faf1cd4fbd",
   "metadata": {},
   "source": [
    "## Prétraitement et découpage des textes afin d’alimenter le moteur RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c01d75ae-1b04-4efe-bc0f-a696e76bca49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 chunks créés (chunk_size=800, overlap=200)\n"
     ]
    }
   ],
   "source": [
    "# Découpage en chunks\n",
    "def chunk_text(text, chunk_size=800, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    L = len(text)\n",
    "    while start < L:\n",
    "        end = min(start + chunk_size, L)\n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(full_text, chunk_size=chunk_size, overlap=chunk_overlap)\n",
    "print(f\"{len(chunks)} chunks créés (chunk_size={chunk_size}, overlap={chunk_overlap})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd332c45-2a65-4098-a8f1-c8cbf2368242",
   "metadata": {},
   "source": [
    "##  Production des vecteurs d’embedding via Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f63e79df-464c-49d1-a99b-8725b9b51e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 41/41 [04:25<00:00,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (41, 4096)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# On s'assure que le serveur Ollama est en cours d’exécution et que le modèle mistral a bien été téléchargé (ollama pull mistral).\n",
    "embeddings = []\n",
    "for chunk in tqdm(chunks, desc=\"Embeddings\"):\n",
    "    resp = ollama.embed(model=model, input=chunk)\n",
    "    \n",
    "    # Le nom de la clé varie selon la version du SDK : embedding pour un vecteur unique ou embeddings pour une liste. Notre code gère les deux.    if \"embeddings\" in resp:\n",
    "    if \"embeddings\" in resp:\n",
    "        emb = resp[\"embeddings\"][0]\n",
    "        \n",
    "    elif \"embedding\" in resp:\n",
    "        emb = resp[\"embedding\"]\n",
    "        \n",
    "    else:\n",
    "        raise RuntimeError(\"Format d'embedding inattendu : %s\" % resp)\n",
    "    embeddings.append(emb)\n",
    "            \n",
    "embeddings = np.array(embeddings, dtype=\"float32\")\n",
    "print(\"Embeddings shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9597f569-bf83-491c-9f47-ab49a735b947",
   "metadata": {},
   "source": [
    "## Génération et stockage de l’index FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "696c109d-0918-4fee-a17d-f158517995c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index sauvegardé: docs.index\n",
      "Chunks sauvegardés: docs.json\n"
     ]
    }
   ],
   "source": [
    "# Indexation FAISS et enregistrement du fichier d’index\n",
    "d = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)  # index simple L2 \n",
    "index.add(embeddings)\n",
    "faiss.write_index(index, index_file)\n",
    "print(\"Index sauvegardé:\", index_file)\n",
    "\n",
    "# 5) Sauvegarde des chunks (id -> texte)\n",
    "with open(docs_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "print(\"Chunks sauvegardés:\", docs_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c7a6fc-3d62-4db1-abd3-172fc5b93fe4",
   "metadata": {},
   "source": [
    "# Notebook interactif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "132e8ab8-6675-4ed7-84ea-3afc5bb8c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger l'index et les chunks\n",
    "index_file = \"/home/sacko/Documents/ChatbotEcoFranceRAG/Scripts/docs.index\"\n",
    "docs_file = \"/home/sacko/Documents/ChatbotEcoFranceRAG//Scripts/docs.json\"\n",
    "model = \"mistral\"\n",
    "index = faiss.read_index(index_file)\n",
    "with open(docs_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    docs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb79dcf-cad4-4919-8aa4-4244e8870d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(text, model=\"mistral\"):\n",
    "    resp = ollama.embed(model=model, input=text)\n",
    "    # gestion de deux formats possibles\n",
    "    if \"embeddings\" in resp:\n",
    "        return np.array(resp[\"embeddings\"][0], dtype=\"float32\")\n",
    "    elif \"embedding\" in resp:\n",
    "        return np.array(resp[\"embedding\"], dtype=\"float32\")\n",
    "    else:\n",
    "        raise RuntimeError(\"Format d'embedding inattendu : %s\" % resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa5790d2-4a10-426f-83d4-ea2fa58df566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query, k):\n",
    "    query_vector = embed_query(query)\n",
    "    query_vector = np.expand_dims(query_vector, axis=0)\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    \n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        results.append({\n",
    "            \"id\": int(idx),\n",
    "            \"distance\": float(dist),\n",
    "            \"text\": chunks[idx]  # chaque chunk est une string\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ef725cc-0a13-4668-a8c4-daf1a231eb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Les facteurs qui influencent l'économie française sont multiples et peuvent être regroupés en plusieurs catégories.\n",
      "\n",
      "Dans le contexte de votre texte, on peut identifier :\n",
      "\n",
      "1. La politique économique allemande d'augmentation de la compétitivité-coût des entreprises en réduisant les cotisations sociales et en augmentant l'imposition sur les biens importés. Cela a pu avoir un impact négatif sur l'économie française, en renforçant les gains de parts de marché allemands au détriment de la France, ce qui s'est traduit par un ralentissement de l'activité dans l'Hexagone.\n",
      "\n",
      "2. Les effets économiques des crises financières, comme celle des années 2008 et 2009, avec des déficits importants et une dette publique considérablement accrue en Europe.\n",
      "\n",
      "3. Les innovations technologiques et la diffusion du modèle de production fordiste dans l'industrie ont également eu un impact important sur l'économie française. Cela a permis d'améliorer la productivité du travail, ce qui a conduit à une croissance économique plus rapide en France que dans les États-Unis.\n",
      "\n",
      "4. Des facteurs spécifiques à la France, comme la reconstruction post-guerre et le rattrapage technologique sur les États-Unis, ont également contribué à son économie.\n",
      "\n",
      "5. La demande des administrations et des ménages a également pu jouer un rôle dans l'évolution de l'économie française en fonction des hausses ou des baisses de salaires réels.\n"
     ]
    }
   ],
   "source": [
    "# Pour construire le prompt\n",
    "query = \"Quels facteurs influencent l'économie française ?\"\n",
    "\n",
    "retrieved = retrieve_context(query, k = 4)\n",
    "\n",
    "retrieved_texts = [r[\"text\"] for r in retrieved]\n",
    "\n",
    "def build_prompt(query, retrieved_chunks):\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "    return f\"Contexte :\\n{context}\\n\\nQuestion : {query}\\nRéponse :\"\n",
    "\n",
    "prompt = build_prompt(query, retrieved_texts)\n",
    "\n",
    "resp = ollama.chat(model=\"mistral\", messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "print(resp[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7bc770-e80f-4da1-ade0-af58c5670ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (RAG)",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
